{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_and_dogs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odus05/Image_Retrieval/blob/master/cats_and_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDLYx4qIx5H7",
        "colab_type": "code",
        "outputId": "92a2f1d4-7645-4412-ce4e-0f507c082f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "import cv2\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import adam\n",
        "from keras import backend as K\n",
        "\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RAGUVVC8RXE",
        "colab_type": "code",
        "outputId": "eee29783-04e2-42de-e95e-d1a4286d8cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5d3o2EyaMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 1. Data Load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoEb9pcyykcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 원본 데이터셋을 압축 해제한 디렉터리 경로\n",
        "original_dataset_cat_dir = '/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs/Cat'\n",
        "original_dataset_dog_dir = '/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs/Dog'\n",
        "\n",
        "# 소규모 데이터셋을 저장할 디렉터리\n",
        "base_dir = '/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small'\n",
        "if os.path.exists(base_dir):  # 반복적인 실행을 위해 디렉토리를 삭제합니다.\n",
        "    shutil.rmtree(base_dir)   # 이 코드는 책에 포함되어 있지 않습니다.\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "# 훈련, 검증, 테스트 분할을 위한 디렉터리\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "# 훈련용 고양이 사진 디렉터리\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "\n",
        "# 훈련용 강아지 사진 디렉터리\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "# 검증용 고양이 사진 디렉터리\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "\n",
        "# 검증용 강아지 사진 디렉터리\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "# 테스트용 고양이 사진 디렉터리\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "\n",
        "# 테스트용 강아지 사진 디렉터리\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "# 처음 1,000개의 고양이 이미지를 train_cats_dir에 복사합니다\n",
        "try:\n",
        "  fnames = ['cat.{}.jpg'.format(i+1) for i in range(1000)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_cat_dir, fname)\n",
        "      dst = os.path.join(train_cats_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "\n",
        "  # 다음 500개 고양이 이미지를 validation_cats_dir에 복사합니다\n",
        "  fnames = ['cat.{}.jpg'.format(i+1) for i in range(1000, 1500)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_cat_dir, fname)\n",
        "      dst = os.path.join(validation_cats_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "      \n",
        "  # 다음 500개 고양이 이미지를 test_cats_dir에 복사합니다\n",
        "  fnames = ['cat.{}.jpg'.format(i+1) for i in range(1500, 2000)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_cat_dir, fname)\n",
        "      dst = os.path.join(test_cats_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "      \n",
        "  # 처음 1,000개의 강아지 이미지를 train_dogs_dir에 복사합니다\n",
        "  fnames = ['dog.{}.jpg'.format(i+1) for i in range(1000)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_dog_dir, fname)\n",
        "      dst = os.path.join(train_dogs_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "      \n",
        "  # 다음 500개 강아지 이미지를 validation_dogs_dir에 복사합니다\n",
        "  fnames = ['dog.{}.jpg'.format(i+1) for i in range(1000, 1500)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_dog_dir, fname)\n",
        "      dst = os.path.join(validation_dogs_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "      \n",
        "  # 다음 500개 강아지 이미지를 test_dogs_dir에 복사합니다\n",
        "  fnames = ['dog.{}.jpg'.format(i+1) for i in range(1500, 2000)]\n",
        "  for fname in fnames:\n",
        "      src = os.path.join(original_dataset_dog_dir, fname)\n",
        "      dst = os.path.join(test_dogs_dir, fname)\n",
        "      shutil.copyfile(src, dst)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O05DQ8B5EFs",
        "colab_type": "code",
        "outputId": "9e47a891-52c2-4b1c-c259-d2cb4c74e2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print('훈련용 고양이 이미지 전체 개수:', len(os.listdir(train_cats_dir)))\n",
        "print('훈련용 강아지 이미지 전체 개수:', len(os.listdir(train_dogs_dir)))\n",
        "print('검증용 고양이 이미지 전체 개수:', len(os.listdir(validation_cats_dir)))\n",
        "print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_dogs_dir)))\n",
        "print('테스트용 고양이 이미지 전체 개수:', len(os.listdir(test_cats_dir)))\n",
        "print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련용 고양이 이미지 전체 개수: 1000\n",
            "훈련용 강아지 이미지 전체 개수: 1000\n",
            "검증용 고양이 이미지 전체 개수: 500\n",
            "검증용 강아지 이미지 전체 개수: 500\n",
            "테스트용 고양이 이미지 전체 개수: 500\n",
            "테스트용 강아지 이미지 전체 개수: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidma1bNQicy",
        "colab_type": "code",
        "outputId": "8ddb829d-a35e-4a12-ccfe-dce4d74cc338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "images = []\n",
        "for root, _, files in os.walk(train_dir):\n",
        "    for fname in files[:3]:\n",
        "        fname = os.path.join(root, fname)\n",
        "        img = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
        "        img = np.asarray(img, dtype='float32') / 255\n",
        "        print('fname: {} | img.shape: {}'.format(fname, img.shape))\n",
        "        img = cv2.resize(img, (150, 150))\n",
        "        images.append(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/cats/cat.1.jpg | img.shape: (281, 300, 3)\n",
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/cats/cat.2.jpg | img.shape: (397, 312, 3)\n",
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/cats/cat.3.jpg | img.shape: (415, 500, 3)\n",
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/dogs/dog.1.jpg | img.shape: (500, 327, 3)\n",
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/dogs/dog.2.jpg | img.shape: (199, 188, 3)\n",
            "fname: /content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/train/dogs/dog.3.jpg | img.shape: (375, 500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7SdLJWHRLJ_",
        "colab_type": "code",
        "outputId": "0c066b88-e941-4947-bd40-0f9e8a0a3261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=adam(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_MP4o4SbUe",
        "colab_type": "code",
        "outputId": "250ff76e-7991-4148-9409-aa19ac42f517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "# 모든 이미지를 1/255로 스케일을 조정합니다\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# 검증 데이터는 증식되어서는 안 됩니다!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # 타깃 디렉터리\n",
        "        train_dir,\n",
        "        # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        # binary_crossentropy 손실을 사용하기 때문에 이진 레이블을 만들어야 합니다\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "CPU times: user 73.1 ms, sys: 2 ms, total: 75.1 ms\n",
            "Wall time: 219 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRFPT6U9THwj",
        "colab_type": "code",
        "outputId": "57bd1c99-b75f-4bd6-c050-87609188a54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_batch, labels_batch = next(train_generator)\n",
        "print('배치 데이터 크기:', data_batch.shape)\n",
        "print('배치 레이블 크기:', labels_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "배치 데이터 크기: (32, 150, 150, 3)\n",
            "배치 레이블 크기: (32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vswJFonZTS4f",
        "colab_type": "code",
        "outputId": "8ea415ed-a40b-4148-8da9-161d982991a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 3,200개의 샘플을 처리할 때까지, 100개의 배치를 뽑는다.\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)\n",
        "model.save('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cats_and_dogs_small_1.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "100/100 [==============================] - 44s 436ms/step - loss: 0.6929 - acc: 0.5181 - val_loss: 0.6892 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 36s 355ms/step - loss: 0.6806 - acc: 0.5594 - val_loss: 0.6711 - val_acc: 0.5625\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.6640 - acc: 0.5841 - val_loss: 0.6436 - val_acc: 0.6009\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 0.6497 - acc: 0.6178 - val_loss: 0.6256 - val_acc: 0.6211\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.6293 - acc: 0.6409 - val_loss: 0.6622 - val_acc: 0.6199\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 38s 379ms/step - loss: 0.6031 - acc: 0.6722 - val_loss: 0.5809 - val_acc: 0.6778\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 0.5942 - acc: 0.6835 - val_loss: 0.5843 - val_acc: 0.6827\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.5927 - acc: 0.6700 - val_loss: 0.6754 - val_acc: 0.6231\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 0.5814 - acc: 0.6903 - val_loss: 0.5361 - val_acc: 0.7126\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 0.5670 - acc: 0.7059 - val_loss: 0.5597 - val_acc: 0.6973\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 0.5557 - acc: 0.7131 - val_loss: 0.5235 - val_acc: 0.7313\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.5534 - acc: 0.7184 - val_loss: 0.5486 - val_acc: 0.7088\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.5593 - acc: 0.7103 - val_loss: 0.5498 - val_acc: 0.7004\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.5431 - acc: 0.7266 - val_loss: 0.5302 - val_acc: 0.7348\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.5300 - acc: 0.7328 - val_loss: 0.5226 - val_acc: 0.7339\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 35s 350ms/step - loss: 0.5347 - acc: 0.7375 - val_loss: 0.5132 - val_acc: 0.7423\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 34s 335ms/step - loss: 0.5432 - acc: 0.7144 - val_loss: 0.4965 - val_acc: 0.7544\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 37s 375ms/step - loss: 0.5231 - acc: 0.7400 - val_loss: 0.4879 - val_acc: 0.7642\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 34s 345ms/step - loss: 0.4926 - acc: 0.7559 - val_loss: 0.4910 - val_acc: 0.7551\n",
            "Epoch 20/100\n",
            " 27/100 [=======>......................] - ETA: 15s - loss: 0.5265 - acc: 0.7326"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSD9TaMT6Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_history(history):\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQZp9OAwkxx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlhJ9izAb83r",
        "colab_type": "text"
      },
      "source": [
        "데이터가 적기 때문에 컨브넷을 처음부터 훈련해서 더 높은 정확도를 달성하기는 어렵습니다. 이런 상황에서 정확도를 높이기 위한 다음 단계는 사전 훈련된 모델을 사용하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLn_yq8Db9eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 특성 추출(Feature Extraction) -> Transfer Learning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFIHKAi1cy_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(150, 150, 3))\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6HX_S5qeI0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 1) 데이터 증식을 사용하지 않는 빠른 특성추출\n",
        "### - conv_base에 데이터를 주입하고 출력을 저장하고 이를 새로운 모델의 입력에 사용합니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJimNkI8fM1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(150, 150), \n",
        "        batch_size=batch_size, \n",
        "        class_mode='binary')\n",
        "    \n",
        "    for i, (inputs_batch, labels_batch) in enumerate(generator):\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size : (i+1) * batch_size] = features_batch\n",
        "        labels[i * batch_size : (i+1) * batch_size] = labels_batch\n",
        "        \n",
        "        if (i+1) * batch_size >= sample_count:\n",
        "            # 제너레이터는 루프 안에서 무한하게 데이터를 만들어내므로 \n",
        "            # 모든 이미지를 한 번씩 처리하고 나면 중지합니다\n",
        "            break\n",
        "    \n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIU7JqMMiqne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, train_labels = extract_features(train_dir, 2000)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
        "test_features, test_labels = extract_features(test_dir, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VOUgo1mjeEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('train_features      : {}, train_labels      : {}'.format(train_features.shape, train_labels.shape))\n",
        "print('validation_features : {}, validation_labels : {}'.format(validation_features.shape, validation_labels.shape))\n",
        "print('test_features       : {}, test_labels : {}'.format(test_features.shape, test_labels.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s80cx8Fjh_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = np.reshape(train_features, (-1, 4*4*512))\n",
        "validation_features = validation_features.reshape([-1, 4*4*512])\n",
        "test_features = np.reshape(test_features, (-1, 4*4*512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL0ynyDDj2Aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=4*4*512))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7qnNAQPkLt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=adam(lr=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=20,\n",
        "                    validation_data = (validation_features, validation_labels))\n",
        "model.save('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cats_and_dogs_small_2.h5')\n",
        "show_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2moAG1ynlDbt",
        "colab_type": "text"
      },
      "source": [
        "약 90%의 검증 정확도에 도달했습니다. 이전 절에서 처음부터 훈련시킨 작은 모델에서 얻은 것보다 훨씬 좋습니다. 하지만 이 그래프는 많은 비율로 드롭아웃을 사용했음에도 불구하고 훈련이 시작하면서 거의 바로 과대적합되고 있다는 것을 보여줍니다. 작은 이미지 데이터셋에서는 과대적합을 막기 위해 필수적인 데이터 증식을 사용하지 않았기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYpoowHTlH7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 2) 데이터 증식을 사용한 특성 추출\n",
        "### - conv_base 모델을 확장하고 입력데이터를 사용해 End-to-End로 실행!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtJSie8_lNni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FetEbToWmcZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('conv_base를 동결하기 전 훈련되는 가중치의 수:', \n",
        "      len(model.trainable_weights))\n",
        "\n",
        "# VGG16 Weight Freezing\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('conv_base를 동결한 후 훈련되는 가중치의 수:', \n",
        "      len(model.trainable_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Skw62-mor2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.1,\n",
        "      zoom_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# 검증 데이터는 증식되어서는 안 됩니다!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # 타깃 디렉터리\n",
        "        train_dir,\n",
        "        # 모든 이미지의 크기를 150 × 150로 변경합니다\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # binary_crossentropy 손실을 사용하므로 이진 레이블이 필요합니다\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=adam(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=1)\n",
        "model.save('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cats_and_dogs_small_3.h5')\n",
        "show_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eJOWfY1n-pM",
        "colab_type": "text"
      },
      "source": [
        "검증 정확도가 이전과 비슷하지만 처음부터 훈련시킨 소규모 컨브넷보다 과대적합이 줄었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko8PZUjhnvNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 미세 조정(Fine-tuning)\n",
        "### - 미세 조정은 특성 추출에 사용했던 동결 모델의 상위 층 몇 개를 동결에서 해제하고 모델에 새로 추가한 층(여기서는 완전 연결 분류기)과 함께 훈련 하는 것입니다.\n",
        "\n",
        "### 미세 조정 방법 ###\n",
        "###1. 사전에 훈련된 기반 네트워크 위에 새로운 네트워크를 추가합니다.\n",
        "###2. 기반 네트워크를 동결합니다.\n",
        "###3. 새로 추가한 네트워크를 훈련합니다.\n",
        "###4. 기반 네트워크에서 일부 층의 동결을 해제합니다.\n",
        "###5. 동결을 해제한 층과 새로 추가한 층을 함께 훈련합니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bc4VmVRn77O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Unfreezing\n",
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXRNKJaop96M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 학습률을 낮추는 이유는 미세 조정하는 세 개의 층에서 학습된 표현을 조금씩 수정하기 위해서입니다. 변경량이 너무 크면 학습된 표현에 나쁜 영향을 끼칠 수 있습니다.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=adam(lr=1e-5),\n",
        "              metrics=['acc'])\n",
        "model.summary()\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps=50)\n",
        "model.save('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cats_and_dogs_small_4.h5')\n",
        "show_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbzJoAbHr0Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 그래프가 불규칙하게 보이기 때문에 지수 이동 평균으로 정확도와 손실값을 부드럽게 표현합니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgtxb8N8r6si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_curve(points, factor=0.8):\n",
        "    smoothed_points = []\n",
        "    for point in points:\n",
        "        if smoothed_points:\n",
        "            previous = smoothed_points[-1]\n",
        "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "        else:\n",
        "            smoothed_points.append(point)\n",
        "    return smoothed_points\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(acc), 'bo', label='Smoothed training acc')\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
        "plt.plot(epochs,\n",
        "         smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKLyT47Xv5dQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('test acc:', test_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGlSD67-wXCW",
        "colab_type": "text"
      },
      "source": [
        "<정리>\n",
        "컨브넷은 컴퓨터 비전 작업에 가장 뛰어난 머신 러닝 모델입니다. 아주 작은 데이터셋에서도 처음부터 훈련해서 괜찮은 성능을 낼 수 있습니다.\n",
        "작은 데이터셋에서는 과대적합이 큰 문제입니다. 데이터 증식은 이미지 데이터를 다룰 때 과대적합을 막을 수 있는 강력한 방법입니다.\n",
        "특성 추출 방식으로 새로운 데이터셋에 기존의 컨브넷을 쉽게 재사용할 수 있습니다. 작은 이미지 데이터셋으로 작업할 때 효과적인 기법입니다.\n",
        "특성 추출을 보완하기 위해 미세 조정을 사용할 수 있습니다. 미세 조정은 기존 모델에서 사전에 학습한 표현의 일부를 새로운 문제에 적응시킵니다. 이 기법은 조금 더 성능을 끌어올립니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCIVRc1-wY6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 컨브넷 학습 시각화 ###\n",
        "###  컨브넷의 표현은 시각적인 개념을 학습한 것이기 때문에 시각화하기 아주 좋습니다.\n",
        "### 1) 컨브넷 중간 층의 출력(중간 층에 있는 활성화)을 시각화하기 : 연속된 컨브넷층이 입력을 어떻게 변형시키는지 이해하고 개별적인 컨브넷 필터의 의미를 파악하는 데 도움이 됩니다.\n",
        "### 2) 컨브넷 필터를 시각화하기 : 컨브넷의 필터가 찾으려는 시각적인 패턴과 개념이 무엇인지 상세하게 이해하는 데 도움이 됩니다.\n",
        "### 3) 클래스 활성화에 대한 히트맵을 이미지에 시각화하기 : 이미지의 어느 부분이 주어진 클래스에 속하는 데 기여했는지 이해하고 이미지에서 객체의 위치를 추정하는 데 도움이 됩니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2mpSYI7wq9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 1) 컨브넷 중간 층의 활성화 시각화하기"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWhCREjK0BH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cats_and_dogs_small_1.h5')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoDsSjdG0Yzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "img_path = '/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/test/cats/cat.1500.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "print(\"img_tensor's shape (before expand_dims) :\", img_tensor.shape)\n",
        "\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "print(\"img_tensor's shape (after expand_dims) :\", img_tensor.shape)\n",
        "\n",
        "# 모델이 훈련될 때 입력에 적용한 전처리 방식을 동일하게 사용합니다\n",
        "img_tensor /= 255.\n",
        "# 이미지 텐서의 크기는 (1, 150, 150, 3)입니다\n",
        "print(img_tensor.shape)\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA-BN5wu1VPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 상위 8개의 층 출력을 추출  (Flatten 전까지)\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "# 입력에 대해 8개 층의 출력을 반환하는 모델을 만듦\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "activation_model.summary()\n",
        "# 이 모델은 하나의 입력(이미지)과 층의 활성화마다 총 8개의 출력을 가집니다.\n",
        "# 층의 활성화마다 하나씩 8개의 넘파이 배열로 이루어진 리스트를 반환한다.\n",
        "activations = activation_model.predict(img_tensor)\n",
        "print(len(activations))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4t-RPEQ2OPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl8BV-VH2RLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 32개의 채널을 가진 148 × 148 크기의 특성 맵입니다. 원본 모델의 첫 번째 층의 활성화 중에서 1 ~ 5 번째 채널을 그려 보겠습니다:\n",
        "fig, axes = plt.subplots(1, 5, figsize=(14,6))\n",
        "pl_images = [\n",
        "             first_layer_activation[0,:,:,0],\n",
        "             first_layer_activation[0,:,:,1],\n",
        "             first_layer_activation[0,:,:,2],\n",
        "             first_layer_activation[0,:,:,3],\n",
        "             first_layer_activation[0,:,:,4]\n",
        "]\n",
        "for i, image in enumerate(pl_images):\n",
        "  axes[i].imshow(image, cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1j_i7hJEie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 층의 이름을 그래프 제목으로 사용합니다\n",
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "# 특성 맵을 그립니다\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # 특성 맵에 있는 특성의 수\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # 특성 맵의 크기는 (1, size, size, n_features)입니다\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # 활성화 채널을 위한 그리드 크기를 구합니다\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # 각 활성화를 하나의 큰 그리드에 채웁니다\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            # 그래프로 나타내기 좋게 특성을 처리합니다\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # 그리드를 출력합니다\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkD1UictJvlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 컨브넷 필터 시각화 하기!!\n",
        "### 특정 합성곱 층의 한 필터의 값을 최대화하는 손실 함수를 정의합니다. 이 활성화 값을 최대화하기 위해 입력 이미지를 변경하도록 확률적 경사 상승법을 사용합니다.\n",
        "### ImageNet에 사전 훈련된 VGG16 네트워크에서 block3_conv1 층의 필터 0번의 활성화를 손실로 정의합니다"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvQl6OZiKOxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "model = VGG16(weights='imagenet',\n",
        "              include_top=False)\n",
        "\n",
        "layer_name = 'block3_conv1'\n",
        "filter_index = 0\n",
        "\n",
        "layer_output = model.get_layer(layer_name).output\n",
        "loss = K.mean(layer_output[:, :, :, filter_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSqkMUVSKU3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 경사 상승법을 구현하기 위해 모델의 입력에 대한 손실의 그래디언트가 필요합니다. 이를 위해 케라스의 backend 모듈에 있는 gradients 함수를 사용하겠습니다:\n",
        "# gradients 함수가 반환하는 텐서 리스트(여기에서는 크기가 1인 리스트)에서 첫 번째 텐서를 추출합니다\n",
        "grads = K.gradients(loss, model.input)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kViUSXIoKdDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 경사 상승법 과정을 부드럽게 하기 위해 사용하는 한 가지 기법은 그래디언트 텐서를 L2 노름(텐서에 있는 값을 제곱합의 제곱근)으로 나누어 정규화하는 것입니다. 이렇게 하면 입력 이미지에 적용할 수정량의 크기를 항상 일정 범위 안에 놓을 수 있습니다.\n",
        "# 0 나눗셈을 방지하기 위해 1e–5을 더합니다\n",
        "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r59t9UzuKjY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 이제 주어진 입력 이미지에 대해 손실 텐서와 그래디언트 텐서를 계산해야 합니다. 케라스 백엔드 함수를 사용하여 처리하겠습니다. iterate는 넘파이 텐서(크기가 1인 텐서의 리스트)를 입력으로 받아 손실과 그래디언트 두 개의 넘파이 텐서를 반환합니다.\n",
        "iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "# 테스트:\n",
        "import numpy as np\n",
        "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwjWdKLKKqlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 여기에서 파이썬 루프를 만들어 확률적 경사 상승법을 구성합니다:\n",
        "# 잡음이 섞인 회색 이미지로 시작합니다\n",
        "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
        "\n",
        "# 업데이트할 그래디언트의 크기\n",
        "step = 1.\n",
        "for i in range(40):   # 경사 상승법을 40회 실행합니다\n",
        "    # 손실과 그래디언트를 계산합니다\n",
        "    loss_value, grads_value = iterate([input_img_data])\n",
        "    # 손실을 최대화하는 방향으로 입력 이미지를 수정합니다\n",
        "    input_img_data += grads_value * step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QpnSLP0KxqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 결과 이미지 텐서는 (1, 150, 150, 3) 크기의 부동 소수 텐서입니다. 이 텐서의 값은 [0, 255] 사이의 정수가 아닙니다. 따라서 출력 가능한 이미지로 변경하기 위해 후처리할 필요가 있습니다. 이를 위해 간단한 함수를 정의해 사용하겠습니다:\n",
        "def deprocess_image(x):\n",
        "    # 텐서의 평균이 0, 표준 편차가 0.1이 되도록 정규화합니다\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # [0, 1]로 클리핑합니다\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # RGB 배열로 변환합니다\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def generate_pattern(layer_name, filter_index, size=150):\n",
        "  # 주어진 층과 필터의 활성화를 최대화하기 위한 손실 함수를 정의합니다\n",
        "  layer_output = model.get_layer(layer_name).output\n",
        "  loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "  # 손실에 대한 입력 이미지의 그래디언트를 계산합니다\n",
        "  grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "  # 그래디언트 정규화\n",
        "  grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "  # 입력 이미지에 대한 손실과 그래디언트를 반환합니다\n",
        "  iterate = K.function([model.input], [loss, grads])\n",
        "  \n",
        "  # 잡음이 섞인 회색 이미지로 시작합니다\n",
        "  input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
        "\n",
        "  # 경사 상승법을 40 단계 실행합니다\n",
        "  step = 1.\n",
        "  for i in range(40):\n",
        "      loss_value, grads_value = iterate([input_img_data])\n",
        "      input_img_data += grads_value * step\n",
        "      \n",
        "  img = input_img_data[0]\n",
        "  return deprocess_image(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhdBmEOK0Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0QZKMvg4P7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 클래스 활성화의 히트맵 시각화 하기!! -> 클래스 활성화 맵(CAM, Class Activation Map) 시각화\n",
        "### 1. 이미지의 어느 부분이 컨브넷의 최종 분류 결정에 기여하는지 이해하는 데 유용합니다.\n",
        "### 2. 분류에 실수가 있는 경우 컨브넷의 결정 과정을 디버깅하는 데 도움이 됩니다.\n",
        "### 3. 또한 이미지에 특정 물체가 있는 위치를 파악하는 데 사용할 수도 있습니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eV8GGUl4ax7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n",
        "### 1) 입력 이미지가 주어지면 합성곱 층에 있는 특성 맵의 출력을 추출합니다.\n",
        "### 2) 그 다음 특성 맵의 모든 채널의 출력에 채널에 대한 클래스의 그래디언트 평균을 곱합니다.\n",
        "### -> '입력 이미지가 각 채널을 활성화하는 정도’에 대한 공간적인 맵을 '클래스에 대한 각 채널의 중요도’로 가중치를 부여하여 '입력 이미지가 클래스를 활성화하는 정도’에 대한 공간적인 맵을 만드는 것입니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSA-6HIw46t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이전 모든 예제에서는 최상단의 완전 연결 분류기를 제외했지만 여기서는 포함합니다\n",
        "model = VGG16(weights='imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsWeognc5IJN",
        "colab_type": "text"
      },
      "source": [
        "이 이미지를 VGG16 모델이 인식할 수 있도록 변환해보죠. 이 모델은 224 × 224 크기의 이미지에서 훈련되었고 keras.applications.vgg16.preprocess_input 함수에 있는 몇 가지 규칙에 따라 전처리 되었습니다. 그러므로 이 이미지를 로드해서 224 × 224 크기로 변경하고 넘파이 float32 텐서로 바꾼 다음 이 전처리 함수를 적용해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_wRSUYN5I4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "# 224 x 224 크기의 파이썬 이미징 라이브러리(PIL) 객체로 반환된다.\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# (224, 224, 3) 형태의 NumPy float32 배열\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# 차원을 추가하여 (1, 224, 224, 3) 형태의 배치로 배열을 변환\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# 데이터를 전처리합니다(채널별 컬러 정규화를 수행합니다)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
        "print(np.argmax(preds[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Adq9dLo67kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측 벡터의 '고양이' 항목\n",
        "cat_output = model.output[:, 386]\n",
        "\n",
        "# VGG16의 마지막 합성곱 층인 block5_conv3 층의 특성 맵\n",
        "last_conv_layer = model.get_layer('block5_conv3')\n",
        "\n",
        "# block5_conv3의 특성 맵 출력에 대한 '아프리카 코끼리' 클래스의 그래디언트\n",
        "grads = K.gradients(cat_output, last_conv_layer.output)[0]\n",
        "\n",
        "# 특성 맵 채널별 그래디언트 평균 값이 담긴 (512,) 크기의 벡터\n",
        "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "# 샘플 이미지가 주어졌을 때 방금 전 정의한 pooled_grads와 block5_conv3의 특성 맵 출력을 구합니다\n",
        "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "# 두 마리 고양이가 있는 샘플 이미지를 주입하고 두 개의 넘파이 배열을 얻습니다\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "# \"고양이\" 클래스에 대한 \"채널의 중요도\"를 특성 맵 배열의 채널에 곱합니다\n",
        "for i in range(512):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "# 만들어진 특성 맵에서 채널 축을 따라 평균한 값이 클래스 활성화의 히트맵입니다\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "\n",
        "# 시각화를 위해 히트맵을 0과 1 사이로 정규화하겠습니다. \n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROS-Z22D8Bev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cv2 모듈을 사용해 원본 이미지를 로드합니다\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# heatmap을 원본 이미지 크기에 맞게 변경합니다\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# heatmap을 RGB 포맷으로 변환합니다\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# 히트맵으로 변환합니다\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "# 0.4는 히트맵의 강도입니다\n",
        "superimposed_img = heatmap * 0.4 + img\n",
        "\n",
        "# 디스크에 이미지를 저장합니다\n",
        "cv2.imwrite('/content/gdrive/My Drive/Colab Notebooks/datasets/cats_and_dogs_small/cat_cam02.jpg', superimposed_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVS94vvyF52S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}